{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome! \n",
    "In this brief tutorial, I will show you how to build a convolutional neural network using Keras. We will train our neural network\n",
    "to recognize handwritten digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of ConvNet](https://raw.githubusercontent.com/NeilNie/LearnConvNets/master/image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset pre-processing\n",
    "Keras has a very nice built in function to download the dataset online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download mnist data and split into train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x141455fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to check the size of the images. They should be 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check image shape\n",
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the model, let's quickly reshape our input data. There are 60,000 images in total. The `1` means that the images\n",
    "don't have color (grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the expected output values to one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the convolutional neural network\n",
    "Finally, we are done with data preprocessing. We can finally start building and training our machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "# add the first conv layer\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "# add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add the second conv layer\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# add a flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# add the first dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# add a dense layer for classification\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        18464     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               495744    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 516,138\n",
      "Trainable params: 516,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been built, let's compile it using some optimizer and loss function and metrics. Once it has been compiled, \n",
    "we can start training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 2.2691 - accuracy: 0.2060 - val_loss: 2.2392 - val_accuracy: 0.3262\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 2.2019 - accuracy: 0.4124 - val_loss: 2.1557 - val_accuracy: 0.4813\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 2.0992 - accuracy: 0.5111 - val_loss: 2.0274 - val_accuracy: 0.5494\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 1.9433 - accuracy: 0.5696 - val_loss: 1.8350 - val_accuracy: 0.6063\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 35s 76ms/step - loss: 1.7207 - accuracy: 0.6371 - val_loss: 1.5765 - val_accuracy: 0.6860\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 1.4494 - accuracy: 0.7129 - val_loss: 1.2942 - val_accuracy: 0.7587\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 36s 76ms/step - loss: 1.1863 - accuracy: 0.7633 - val_loss: 1.0507 - val_accuracy: 0.7931\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 36s 78ms/step - loss: 0.9777 - accuracy: 0.7907 - val_loss: 0.8723 - val_accuracy: 0.8139\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 37s 80ms/step - loss: 0.8300 - accuracy: 0.8083 - val_loss: 0.7490 - val_accuracy: 0.8301\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.7271 - accuracy: 0.8228 - val_loss: 0.6629 - val_accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "model_log = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_MNIST_model/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('my_MNIST_model')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = load_model('my_MNIST_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load an image of a number that we want the model to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "new_image = cv2.imread(\"./my_image_1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "new_image = cv2.resize(new_image, (28, 28))\n",
    "\n",
    "print(new_image.shape)\n",
    "plt.imshow(new_image)\n",
    "\n",
    "new_image = np.expand_dims(new_image, 2)\n",
    "image_data = np.expand_dims(new_image, 0)\n",
    "\n",
    "new_image = new_image.astype(np.float32)\n",
    "new_image /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(image_data)[0]\n",
    "print(pred[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
